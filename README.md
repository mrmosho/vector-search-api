# Hybrid Search System

A sophisticated search system that combines semantic and keyword search capabilities with Arabic-English language support. The system uses sentence transformers for semantic understanding and TF-IDF for exact keyword matching, with intelligent query analysis to optimize search strategies.

## ğŸš€ Features

- **Hybrid Search**: Combines semantic (FAISS) and keyword (TF-IDF) search
- **Smart Query Analysis**: Automatically detects short queries (like "COMI") and adjusts search strategy
- **Arabic-English Support**: Handles multilingual content with proper text processing
- **REST API**: FastAPI-based REST endpoints for easy integration
- **Modular Architecture**: Clean, maintainable code following Single Responsibility Principle
- **Caching System**: Intelligent caching of embeddings and indices for performance
- **CLI Interface**: Command-line interface for direct interaction

## ğŸ—ï¸ Architecture

The system follows a modular architecture with clear separation of concerns:

```
â”œâ”€â”€ Data Layer (data_loader.py, text_processor.py)
â”œâ”€â”€ Model Layer (model_manager.py, file_manager.py)
â”œâ”€â”€ Search Layer (semantic_index.py, keyword_index.py)
â”œâ”€â”€ Logic Layer (query_analyzer.py, hybrid_searcher.py)
â”œâ”€â”€ Presentation Layer (result_formatter.py, api_*)
â””â”€â”€ Interface Layer (main.py, run_api.py)
```

## ğŸ“‹ Prerequisites

- Python 3.8+
- 4GB+ RAM (for model loading)
- CSV data file with `TITLE` and `DESCRIPTION` columns

## ğŸ› ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/hybrid-search-system.git
   cd hybrid-search-system
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   # For CLI only
   pip install -r requirements.txt
   
   # For CLI + API
   pip install -r requirements_api.txt
   ```

4. **Prepare your data**
   - Place your CSV file as `data.csv` in the project root
   - Ensure it has `TITLE` and `DESCRIPTION` columns
   - Optional: `MOD_DATE`, `SOURCE_NAME` columns for metadata

## ğŸš€ Quick Start

### CLI Usage

```bash
# Start interactive search
python main.py

# Search directly
ğŸ’¬ Search: COMI
```

### API Usage

```bash
# Start API server
python run_api.py --csv data.csv --port 8000

# Health check
curl http://localhost:8000/health

# Search
curl "http://localhost:8000/search?q=COMI&top_k=5"
```

## ğŸ“– Usage Examples

### 1. Basic Search
```bash
# CLI
python main.py
ğŸ’¬ Search: financial analysis

# API
curl "http://localhost:8000/search?q=financial%20analysis&top_k=10"
```

### 2. Short Query (Acronym) Search
```bash
# The system automatically detects "COMI" as a short query
# and uses 70% keyword + 30% semantic weighting
ğŸ’¬ Search: COMI
```

### 3. API Response Format
```json
{
  "query": "COMI",
  "strategy": "30% semantic + 70% keyword (keyword-focused)",
  "total_results": 5,
  "results": [
    {
      "result_number": 1,
      "title": "COMI Project Update",
      "date": "2024-01-15",
      "source": "Internal",
      "summary": "Latest updates on the COMI initiative...",
      "score": 0.8542
    }
  ],
  "execution_time_ms": 45.6
}
```

## ğŸ”§ Configuration

### Model Configuration
Edit `model_manager.py` to change the sentence transformer model:
```python
MODEL_OPTIONS = [
    "sentence-transformers/all-MiniLM-L6-v2",  # Default: Fast, good quality
    "sentence-transformers/all-mpnet-base-v2", # Better quality, slower
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # Multilingual
]
```

### Search Strategy
Modify `query_analyzer.py` to adjust search weighting:
```python
def get_search_weights(query):
    if QueryAnalyzer.is_short_query(query):
        return {
            'semantic_weight': 0.3,  # Adjust these weights
            'keyword_weight': 0.7,
            'strategy': 'keyword-focused'
        }
```

## ğŸ“ Project Structure

```
hybrid-search-system/
â”œâ”€â”€ ğŸ“Š Data Processing
â”‚   â”œâ”€â”€ data_loader.py          # CSV data loading and validation
â”‚   â””â”€â”€ text_processor.py       # Text cleaning and preprocessing
â”œâ”€â”€ ğŸ¤– Model Management
â”‚   â”œâ”€â”€ model_manager.py        # Sentence transformer management
â”‚   â””â”€â”€ file_manager.py         # File I/O and caching
â”œâ”€â”€ ğŸ” Search Engines
â”‚   â”œâ”€â”€ semantic_index.py       # FAISS semantic search
â”‚   â””â”€â”€ keyword_index.py        # TF-IDF keyword search
â”œâ”€â”€ ğŸ§  Intelligence Layer
â”‚   â”œâ”€â”€ query_analyzer.py       # Query analysis and strategy
â”‚   â”œâ”€â”€ hybrid_searcher.py      # Search coordination
â”‚   â””â”€â”€ result_formatter.py     # Result formatting
â”œâ”€â”€ ğŸŒ API Layer
â”‚   â”œâ”€â”€ api_models.py           # Pydantic models
â”‚   â”œâ”€â”€ api_routes.py           # FastAPI route handlers
â”‚   â”œâ”€â”€ api_server.py           # FastAPI application
â”‚   â”œâ”€â”€ search_service.py       # Search business logic
â”‚   â””â”€â”€ run_api.py              # API server runner
â”œâ”€â”€ ğŸ–¥ï¸ Interface
â”‚   â””â”€â”€ main.py                 # CLI interface
â”œâ”€â”€ ğŸ“‹ Documentation
â”‚   â”œâ”€â”€ README.md               # This file
â”‚   â”œâ”€â”€ diagrams/               # System diagrams
â”‚   â””â”€â”€ flowchart.md            # Process flowcharts
â””â”€â”€ âš™ï¸ Configuration
    â”œâ”€â”€ requirements.txt        # Core dependencies
    â”œâ”€â”€ requirements_api.txt    # API dependencies
    â””â”€â”€ .gitignore             # Git ignore rules
```

## ğŸ”„ How It Works

### 1. Smart Query Analysis
- **Short queries** (â‰¤6 chars, like "COMI"): 70% keyword + 30% semantic
- **Long queries**: 70% semantic + 30% keyword
- Prevents issues like "COMI" being interpreted as Spanish "comÃ­"

### 2. Parallel Search Execution
```mermaid
graph LR
    A[Query] --> B[Query Analyzer]
    B --> C[Semantic Search]
    B --> D[Keyword Search]
    C --> E[Result Combiner]
    D --> E
    E --> F[Ranked Results]
```

### 3. Intelligent Caching
- **Embeddings**: Cached as `embeddings.npy`
- **FAISS Index**: Cached as `faiss.index`
- **TF-IDF Matrix**: Cached as `tfidf_matrix.npz`
- **Vocabulary**: Cached as `tfidf_vocab.npy`

## ğŸš€ API Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/health` | GET | System health check | None |
| `/search` | GET | Search documents | `q` (query), `top_k` (limit) |

### Health Check Response
```json
{
  "status": "healthy",
  "message": "Search system is running",
  "documents_loaded": 36197,
  "search_capabilities": {
    "semantic_available": true,
    "keyword_available": true,
    "hybrid_mode": true
  }
}
```

## ğŸ”§ Troubleshooting

### Common Issues

1. **Model Download Fails**
   ```bash
   # Clear cache and retry
   rm -rf ~/.cache/huggingface/
   python main.py
   ```

2. **Memory Error**
   - Reduce `max_features` in `keyword_index.py`
   - Use smaller sentence transformer model
   - Process data in smaller batches

3. **No Search Results**
   - Check CSV file has required columns
   - Verify data is not empty
   - Check health endpoint: `/health`

4. **Import Errors**
   ```bash
   # Debug imports
   python debug_imports.py
   ```

### Performance Optimization

- **For large datasets (100K+ docs)**: Use `faiss-gpu` instead of `faiss-cpu`
- **For memory constraints**: Reduce `max_features` in TF-IDF
- **For speed**: Use smaller embedding models

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
pip install -r requirements_api.txt

# Run tests
python -m pytest tests/

# Run linting
flake8 .
black .
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Sentence Transformers** for semantic embeddings
- **FAISS** for efficient similarity search
- **FastAPI** for the REST API framework
- **scikit-learn** for TF-IDF implementation

## ğŸ“ Support

- ğŸ“§ Email: your.email@example.com
- ğŸ› Issues: [GitHub Issues](https://github.com/yourusername/hybrid-search-system/issues)
- ğŸ“– Documentation: [Wiki](https://github.com/yourusername/hybrid-search-system/wiki)

## ğŸ—ºï¸ Roadmap

- [ ] Support for more languages (French, German, etc.)
- [ ] Real-time indexing for dynamic data
- [ ] Advanced query preprocessing
- [ ] Integration with Elasticsearch
- [ ] Web-based UI interface
- [ ] Docker containerization
- [ ] Kubernetes deployment configs

---

**Made with â¤ï¸ for better search experiences**